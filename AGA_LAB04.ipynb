{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Restricted Boltzmann Machine (RBM)**"
      ],
      "metadata": {
        "id": "E0SRIpYuRQfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL5fXV7qPMu3",
        "outputId": "9740f8ab-f4ad-4ac0-a9fe-e06afc2207e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0133\n",
            "Epoch 2, Loss: 0.0133\n",
            "Epoch 3, Loss: 0.0133\n",
            "Epoch 4, Loss: 0.0133\n",
            "Epoch 5, Loss: 0.0133\n",
            "Epoch 6, Loss: 0.0133\n",
            "Epoch 7, Loss: 0.0133\n",
            "Epoch 8, Loss: 0.0133\n",
            "Epoch 9, Loss: 0.0133\n",
            "Epoch 10, Loss: 0.0133\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int64)\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, visible_units, hidden_units, learning_rate=0.01, epochs=10, batch_size=64):\n",
        "        self.visible_units = visible_units\n",
        "        self.hidden_units = hidden_units\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = tf.Variable(tf.random.normal([self.visible_units, self.hidden_units], stddev=0.01))\n",
        "        self.hidden_bias = tf.Variable(tf.zeros([self.hidden_units]))\n",
        "        self.visible_bias = tf.Variable(tf.zeros([self.visible_units]))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return tf.nn.sigmoid(x)\n",
        "\n",
        "    def sample(self, probabilities):\n",
        "        return tf.nn.relu(tf.sign(probabilities - tf.random.uniform(tf.shape(probabilities))))\n",
        "\n",
        "    def forward(self, v):\n",
        "        h_prob = self.sigmoid(tf.matmul(v, self.weights) + self.hidden_bias)\n",
        "        h_sample = self.sample(h_prob)\n",
        "        return h_prob, h_sample\n",
        "\n",
        "    def backward(self, h):\n",
        "        v_prob = self.sigmoid(tf.matmul(h, tf.transpose(self.weights)) + self.visible_bias)\n",
        "        return v_prob\n",
        "\n",
        "    def train(self, data):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(data).batch(self.batch_size)\n",
        "\n",
        "        optimizer = tf.optimizers.Adam(self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for batch in dataset:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    # Positive phase\n",
        "                    h_prob, h_sample = self.forward(batch)\n",
        "                    pos_associations = tf.matmul(tf.transpose(batch), h_prob)\n",
        "\n",
        "                    # Negative phase\n",
        "                    v_reconstructed = self.backward(h_sample)\n",
        "                    h_reconstructed, _ = self.forward(v_reconstructed)\n",
        "                    neg_associations = tf.matmul(tf.transpose(v_reconstructed), h_reconstructed)\n",
        "\n",
        "                    # Compute gradients\n",
        "                    weight_gradient = (pos_associations - neg_associations) / self.batch_size\n",
        "                    visible_bias_gradient = tf.reduce_mean(batch - v_reconstructed, axis=0)\n",
        "                    hidden_bias_gradient = tf.reduce_mean(h_prob - h_reconstructed, axis=0)\n",
        "\n",
        "                    loss = tf.reduce_mean(tf.square(batch - v_reconstructed))\n",
        "                    total_loss += loss.numpy()\n",
        "\n",
        "                # Update weights and biases\n",
        "                gradients = [weight_gradient, hidden_bias_gradient, visible_bias_gradient]\n",
        "                optimizer.apply_gradients(zip(gradients, [self.weights, self.hidden_bias, self.visible_bias]))\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data):.4f}\")\n",
        "\n",
        "    def transform(self, data):\n",
        "        h_prob, _ = self.forward(data)\n",
        "        return h_prob.numpy()\n",
        "\n",
        "# Initialize and train the RBM\n",
        "rbm = RBM(visible_units=784, hidden_units=128, learning_rate=0.01, epochs=10, batch_size=64)\n",
        "rbm.train(X_train)\n",
        "\n",
        "# Extract features from training and test data\n",
        "X_train_features = rbm.transform(X_train)\n",
        "X_test_features = rbm.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression model on extracted features\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_features, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_features)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Classification Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFC-fpfaW0Gw",
        "outputId": "ec10ca47-98bd-4c18-e915-c6626f1e77d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 0.1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize first 16 feature detectors (weights)\n",
        "fig, axes = plt.subplots(4, 4, figsize=(5, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(rbm.weights.numpy()[:, i].reshape(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle(\"RBM Learned Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "f1MsRXlnW19P",
        "outputId": "7d05f8d5-f715-4129-bd32-4ffccd6c2501"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Stacked RBM**"
      ],
      "metadata": {
        "id": "xByP4JEeWNHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, visible_units, hidden_units, learning_rate=0.1, epochs=500):\n",
        "        self.visible_units = visible_units\n",
        "        self.hidden_units = hidden_units\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.normal(0, 0.01, (self.visible_units, self.hidden_units))\n",
        "        self.hidden_bias = np.zeros((1, self.hidden_units))\n",
        "        self.visible_bias = np.zeros((1, self.visible_units))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, probabilities):\n",
        "        return (np.random.rand(*probabilities.shape) < probabilities).astype(np.float32)\n",
        "\n",
        "    def train(self, data):\n",
        "        for epoch in range(self.epochs):\n",
        "            # Positive phase\n",
        "            pos_hidden_probs = self.sigmoid(np.dot(data, self.weights) + self.hidden_bias)\n",
        "            pos_hidden_states = self.sample(pos_hidden_probs)\n",
        "            pos_associations = np.dot(data.T, pos_hidden_probs)\n",
        "\n",
        "            # Negative phase\n",
        "            neg_visible_probs = self.sigmoid(np.dot(pos_hidden_states, self.weights.T) + self.visible_bias)\n",
        "            neg_visible_states = self.sample(neg_visible_probs)\n",
        "            neg_hidden_probs = self.sigmoid(np.dot(neg_visible_states, self.weights) + self.hidden_bias)\n",
        "            neg_associations = np.dot(neg_visible_states.T, neg_hidden_probs)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights += self.learning_rate * ((pos_associations - neg_associations) / data.shape[0])\n",
        "            self.visible_bias += self.learning_rate * np.mean(data - neg_visible_states, axis=0, keepdims=True)\n",
        "            self.hidden_bias += self.learning_rate * np.mean(pos_hidden_probs - neg_hidden_probs, axis=0, keepdims=True)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                error = np.mean((data - neg_visible_states) ** 2)\n",
        "                print(f\"Epoch {epoch}: Reconstruction Error = {error:.4f}\")\n",
        "\n",
        "    def transform(self, data):\n",
        "        return self.sigmoid(np.dot(data, self.weights) + self.hidden_bias)\n",
        "\n",
        "\n",
        "class StackedRBM:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.1, epochs=500):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.rbms = []\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize each RBM in the stack\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.rbms.append(RBM(layer_sizes[i], layer_sizes[i + 1], learning_rate, epochs))\n",
        "\n",
        "    def pretrain(self, data):\n",
        "        layer_input = data\n",
        "        for i, rbm in enumerate(self.rbms):\n",
        "            print(f\"\\nTraining RBM Layer {i + 1}: {rbm.visible_units} -> {rbm.hidden_units}\")\n",
        "            rbm.train(layer_input)\n",
        "            layer_input = rbm.transform(layer_input)  # Pass transformed data to the next layer\n",
        "\n",
        "    def transform(self, data):\n",
        "        for rbm in self.rbms:\n",
        "            data = rbm.transform(data)\n",
        "        return data\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate dummy data: 10 samples, 8 features\n",
        "    data = np.random.randint(0, 2, (10, 8)).astype(np.float32)\n",
        "\n",
        "    # Define stacked RBM architecture: 8 -> 6 -> 4 hidden units\n",
        "    srmb = StackedRBM(layer_sizes=[8, 6, 4], learning_rate=0.1, epochs=500)\n",
        "\n",
        "    # Pretrain the stacked RBM\n",
        "    srmb.pretrain(data)\n",
        "\n",
        "    print(\"\\nFinal Transformed Data:\")\n",
        "    print(srmb.transform(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHHDWNr2WTb_",
        "outputId": "7860100e-67da-4f9c-97ae-f3717540159b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training RBM Layer 1: 8 -> 6\n",
            "Epoch 0: Reconstruction Error = 0.5125\n",
            "Epoch 100: Reconstruction Error = 0.5375\n",
            "Epoch 200: Reconstruction Error = 0.6000\n",
            "Epoch 300: Reconstruction Error = 0.4625\n",
            "Epoch 400: Reconstruction Error = 0.4500\n",
            "\n",
            "Training RBM Layer 2: 6 -> 4\n",
            "Epoch 0: Reconstruction Error = 0.2725\n",
            "Epoch 100: Reconstruction Error = 0.2675\n",
            "Epoch 200: Reconstruction Error = 0.2782\n",
            "Epoch 300: Reconstruction Error = 0.3110\n",
            "Epoch 400: Reconstruction Error = 0.2951\n",
            "\n",
            "Final Transformed Data:\n",
            "[[0.49820654 0.49848738 0.49855734 0.49874819]\n",
            " [0.51535009 0.51631715 0.51600802 0.51676375]\n",
            " [0.49411806 0.49423012 0.49442964 0.49448911]\n",
            " [0.50426403 0.50480535 0.50478594 0.50519793]\n",
            " [0.50985984 0.51061733 0.51048567 0.51107679]\n",
            " [0.50132473 0.50172869 0.5017783  0.5020732 ]\n",
            " [0.51914091 0.52026332 0.51988972 0.52077927]\n",
            " [0.51005128 0.51080861 0.51063101 0.51121464]\n",
            " [0.51668747 0.51770733 0.5173928  0.51819912]\n",
            " [0.52013247 0.5213002  0.52088905 0.52180933]]\n"
          ]
        }
      ]
    }
  ]
}